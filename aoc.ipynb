{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "DATA_ROOT = Path(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = DATA_ROOT / 'day_1' / 'full.txt'\n",
    "contents = np.genfromtxt(file, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941353"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[:, 0].sort()\n",
    "contents[:, 1].sort()\n",
    "np.abs(np.diff(contents, axis=1)).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22539317"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers, counts = np.unique(contents[:, 1], return_counts=True)\n",
    "occurences = dict(zip(numbers, counts))\n",
    "s = np.sum(contents[:, 0] * np.vectorize(lambda x: occurences.get(x, 0))(contents[:, 0]))\n",
    "s.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = DATA_ROOT / 'day_2' / 'full.txt'\n",
    "contents = [[int(x) for x in l.split()] for l in open(file, 'r').readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for report in contents:\n",
    "    report = np.array(report)\n",
    "    ascending = np.all(report[:-1] <= report[1:])\n",
    "    descending = np.all(report[:-1] >= report[1:])\n",
    "    differences = np.abs(np.diff(report))\n",
    "    level_changes = (differences.min() >= 1) & (differences.max() <= 3)\n",
    "    s += (ascending | descending) & level_changes\n",
    "s.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_report(report):\n",
    "    ascending = np.all(report[:-1] <= report[1:])\n",
    "    descending = np.all(report[:-1] >= report[1:])\n",
    "    differences = np.abs(np.diff(report))\n",
    "    level_changes = (differences.min() >= 1) & (differences.max() <= 3)\n",
    "    return (ascending | descending) & level_changes\n",
    "\n",
    "s = 0\n",
    "for report in contents:\n",
    "    report = np.array(report)\n",
    "    if check_report(report):\n",
    "        s += 1\n",
    "    else:\n",
    "        for i in range(len(report)):\n",
    "            if check_report(np.concatenate((report[:i], report[i + 1:]))):\n",
    "                s += 1\n",
    "                break\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = DATA_ROOT / 'day_3' / 'full.txt'\n",
    "contents = ''.join(open(file, 'r').readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((int(s.split(',')[0][4:]) * int(s.split(',')[1][:-1]) for s in re.findall(r'mul\\(\\d{1,3},\\d{1,3}\\)', contents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76911921"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = re.findall(r'mul\\(\\d{1,3},\\d{1,3}\\)|do\\(\\)|don\\'t\\(\\)', contents)\n",
    "enabled = True\n",
    "s = 0\n",
    "for match in matches:\n",
    "    if match == 'do()':\n",
    "        enabled = True\n",
    "    elif match == 'don\\'t()':\n",
    "        enabled = False\n",
    "    elif enabled:\n",
    "        s += int(match.split(',')[0][4:]) * int(match.split(',')[1][:-1])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word(arr, row, col, word='XMAS'):\n",
    "    nrows, ncols = arr.shape\n",
    "    word_size = len(word)\n",
    "    word = np.array(list(word))\n",
    "    word_backwards = word[::-1]\n",
    "\n",
    "    count = 0\n",
    "    if col <= nrows - word_size:\n",
    "        hor_subword = arr[row, col:col + word_size]\n",
    "        if  np.array_equal(hor_subword, word) or np.array_equal(hor_subword, word_backwards):\n",
    "            count += 1\n",
    "        \n",
    "    if row <= ncols - word_size:\n",
    "        vert_subword = arr[row:row + word_size, col]\n",
    "        if np.array_equal(vert_subword, word) or np.array_equal(vert_subword, word_backwards):\n",
    "            count += 1\n",
    "\n",
    "    if row <= nrows - word_size and col <= ncols - word_size:\n",
    "        diag_subword = np.diagonal(arr[row: row + word_size, col: col + word_size])\n",
    "        if np.array_equal(diag_subword, word) or np.array_equal(diag_subword, word_backwards):\n",
    "            count += 1\n",
    "\n",
    "    if row >= word_size - 1 and col <= ncols - word_size:\n",
    "        diag_subword = np.diagonal(np.fliplr(arr[row - word_size + 1: row + 1, col: col + word_size]))\n",
    "        if np.array_equal(diag_subword, word) or np.array_equal(diag_subword, word_backwards):\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "def check_cross(arr, row, col):\n",
    "    if arr[row, col] != 'A':\n",
    "        return False\n",
    "    tl = arr[row - 1, col - 1]\n",
    "    tr = arr[row - 1, col + 1]\n",
    "    bl = arr[row + 1, col - 1]\n",
    "    br = arr[row + 1, col + 1]\n",
    "    return  ((tl == 'M' and br == 'S') or (tl == 'S' and br == 'M')) \\\n",
    "        and ((bl == 'M' and tr == 'S') or (bl == 'S' and tr == 'M'))\n",
    "\n",
    "file = DATA_ROOT / 'day_4' / 'test.txt'\n",
    "contents = np.genfromtxt(file, dtype=str, delimiter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2578"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrows, ncols = contents.shape\n",
    "np.sum([[check_word(contents, row, col) for col in range(ncols)] for row in range(nrows)]).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 140)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrows, ncols = contents.shape\n",
    "np.sum([[check_cross(arr, row, col) for col in range(1, ncols - 1)] for row in range(1, nrows - 1)]).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
